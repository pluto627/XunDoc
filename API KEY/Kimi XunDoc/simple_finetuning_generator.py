#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆKimiå¾®è°ƒæ•°æ®ç”Ÿæˆå™¨
ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½ï¼šæ•°æ®æ ¼å¼è½¬æ¢å’ŒQAå¯¹ç”Ÿæˆ
"""

import os
import json
import random
from openai import OpenAI
from datetime import datetime

# APIé…ç½®
API_KEY = "sk-wc9tcOA0q2ShfOjKZ91YStbbyyoMGpiegf6zq54i9kpaQtke"

client = OpenAI(
    api_key=API_KEY,
    base_url="https://api.moonshot.cn/v1",
)

def load_medical_data():
    """åŠ è½½åŒ»å­¦æ•°æ®"""
    print("ğŸ“¥ æ­£åœ¨åŠ è½½åŒ»å­¦æ•°æ®...")
    
    # åŠ è½½å¯¹è¯æ ¼å¼æ•°æ®
    chat_file = "jmed_data/jmed_chat_format.jsonl"
    chat_data = []
    
    try:
        with open(chat_file, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= 20:  # åªå–å‰20æ¡ä½œä¸ºç¤ºä¾‹
                    break
                line = line.strip()
                if line:
                    chat_data.append(json.loads(line))
        print(f"âœ… åŠ è½½äº† {len(chat_data)} æ¡å¯¹è¯æ•°æ®")
    except Exception as e:
        print(f"âŒ åŠ è½½å¯¹è¯æ•°æ®å¤±è´¥: {e}")
    
    return chat_data

def convert_to_alpaca_format(chat_data):
    """è½¬æ¢ä¸ºAlpacaæ ¼å¼"""
    print("ğŸ”„ è½¬æ¢ä¸ºAlpacaæ ¼å¼...")
    alpaca_data = []
    
    for i, item in enumerate(chat_data):
        if "messages" in item and len(item["messages"]) >= 2:
            user_msg = item["messages"][0]["content"]
            assistant_msg = item["messages"][1]["content"]
            
            # å¤„ç†åŒ»å­¦å¤šé€‰é¢˜
            if "ä»¥ä¸‹æ˜¯ä¸€é“åŒ»å­¦å¤šé€‰é¢˜" in user_msg:
                instruction = "è¯·åˆ†æåŒ»å­¦å¤šé€‰é¢˜å¹¶ç»™å‡ºæ­£ç¡®ç­”æ¡ˆå’Œè¯¦ç»†è§£é‡Šã€‚"
                # æå–é¢˜ç›®å†…å®¹
                input_text = user_msg.replace("ä»¥ä¸‹æ˜¯ä¸€é“åŒ»å­¦å¤šé€‰é¢˜ï¼Œè¯·ä»”ç»†åˆ†æåé€‰æ‹©æ­£ç¡®ç­”æ¡ˆï¼š\n\n", "")
                output_text = assistant_msg
            else:
                instruction = "è¯·å›ç­”åŒ»å­¦ç›¸å…³é—®é¢˜ã€‚"
                input_text = user_msg
                output_text = assistant_msg
            
            alpaca_item = {
                "instruction": instruction,
                "input": input_text[:1000],  # é™åˆ¶é•¿åº¦
                "output": output_text,
                "id": f"medical_{i+1}",
                "source": "jmed_chat_format"
            }
            alpaca_data.append(alpaca_item)
    
    print(f"âœ… è½¬æ¢äº† {len(alpaca_data)} æ¡Alpacaæ ¼å¼æ•°æ®")
    return alpaca_data

def generate_enhanced_qa(sample_case):
    """ç”Ÿæˆå¢å¼ºçš„QAå¯¹"""
    try:
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå¢å¼ºQAå¯¹...")
        
        system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åŒ»å­¦æ•™è‚²ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„åŒ»å­¦æ¡ˆä¾‹ç”Ÿæˆ3ä¸ªä¸åŒè§’åº¦çš„é—®ç­”å¯¹ã€‚

è¦æ±‚ï¼š
1. é—®é¢˜è¦æœ‰æ•™è‚²ä»·å€¼ï¼Œæ¶µç›–ç—‡çŠ¶åˆ†æã€è¯Šæ–­æ€è·¯ã€æ²»ç–—å»ºè®®ç­‰è§’åº¦
2. ç­”æ¡ˆè¦è¯¦ç»†ã€å‡†ç¡®ï¼ŒåŒ…å«åŒ»å­¦åŸç†
3. ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«questionsæ•°ç»„

ç¤ºä¾‹æ ¼å¼ï¼š
{
  "questions": [
    {
      "question": "é—®é¢˜1",
      "answer": "è¯¦ç»†ç­”æ¡ˆ1"
    }
  ]
}"""

        # æå–æ¡ˆä¾‹æ–‡æœ¬
        if "æ‚£è€…" in sample_case:
            case_start = sample_case.find("æ‚£è€…")
            case_end = sample_case.find("æ ¹æ®æ‚£è€…çš„ç—‡çŠ¶")
            if case_start != -1 and case_end != -1:
                case_text = sample_case[case_start:case_end].strip()
            else:
                case_text = sample_case[:800]
        else:
            case_text = sample_case[:800]

        user_prompt = f"è¯·åŸºäºä»¥ä¸‹åŒ»å­¦æ¡ˆä¾‹ç”Ÿæˆ3ä¸ªæ•™è‚²æ€§é—®ç­”å¯¹ï¼š\n\n{case_text}"

        completion = client.chat.completions.create(
            model="moonshot-v1-128k",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(completion.choices[0].message.content)
        questions = result.get("questions", [])
        
        enhanced_data = []
        for i, qa in enumerate(questions):
            enhanced_item = {
                "instruction": "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦æ•™è‚²é—®é¢˜ã€‚",
                "input": qa.get("question", ""),
                "output": qa.get("answer", ""),
                "id": f"enhanced_{i+1}",
                "source": "kimi_generated"
            }
            enhanced_data.append(enhanced_item)
        
        print(f"âœ… ç”Ÿæˆäº† {len(enhanced_data)} æ¡å¢å¼ºQAå¯¹")
        return enhanced_data
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¢å¼ºQAå¯¹å¤±è´¥: {e}")
        return []

def create_domain_qa():
    """åˆ›å»ºé¢†åŸŸä¸“ä¸šQAå¯¹"""
    try:
        print("ğŸ¥ æ­£åœ¨ç”Ÿæˆä¸“ä¸šé¢†åŸŸQAå¯¹...")
        
        system_prompt = """ä½ æ˜¯ä¸€ä½åŒ»å­¦è¯Šæ–­ä¸“å®¶ã€‚è¯·ç”Ÿæˆ5ä¸ªå…³äºåŒ»å­¦è¯Šæ–­çš„ä¸“ä¸šé—®ç­”å¯¹ã€‚

è¦æ±‚ï¼š
1. é—®é¢˜è¦æ¶µç›–ä¸åŒçš„è¯Šæ–­åœºæ™¯
2. ç­”æ¡ˆè¦åŒ…å«è¯Šæ–­æ€è·¯å’Œä¸´åºŠç»éªŒ
3. å…·æœ‰å®ç”¨ä»·å€¼
4. ä»¥JSONæ ¼å¼è¿”å›

ç¤ºä¾‹æ ¼å¼ï¼š
{
  "questions": [
    {
      "question": "å¦‚ä½•é‰´åˆ«è¯Šæ–­æ€¥æ€§è…¹ç—›ï¼Ÿ",
      "answer": "æ€¥æ€§è…¹ç—›çš„é‰´åˆ«è¯Šæ–­éœ€è¦..."
    }
  ]
}"""

        completion = client.chat.completions.create(
            model="moonshot-v1-128k",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "è¯·ç”Ÿæˆ5ä¸ªåŒ»å­¦è¯Šæ–­ä¸“ä¸šé—®ç­”å¯¹"}
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(completion.choices[0].message.content)
        questions = result.get("questions", [])
        
        domain_data = []
        for i, qa in enumerate(questions):
            domain_item = {
                "instruction": "è¯·å›ç­”åŒ»å­¦è¯Šæ–­ä¸“ä¸šé—®é¢˜ã€‚",
                "input": qa.get("question", ""),
                "output": qa.get("answer", ""),
                "id": f"domain_{i+1}",
                "source": "domain_expert"
            }
            domain_data.append(domain_item)
        
        print(f"âœ… ç”Ÿæˆäº† {len(domain_data)} æ¡ä¸“ä¸šQAå¯¹")
        return domain_data
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆä¸“ä¸šQAå¯¹å¤±è´¥: {e}")
        return []

def save_jsonl(data, filename):
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    os.makedirs("finetuning_output", exist_ok=True)
    filepath = f"finetuning_output/{filename}"
    
    with open(filepath, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"ğŸ’¾ ä¿å­˜äº† {len(data)} æ¡æ•°æ®åˆ°: {filepath}")
    return filepath

def generate_stats(all_data):
    """ç”Ÿæˆæ•°æ®ç»Ÿè®¡"""
    stats = {
        "total_samples": len(all_data),
        "sources": {},
        "avg_input_length": 0,
        "avg_output_length": 0,
        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    input_lengths = []
    output_lengths = []
    
    for item in all_data:
        source = item.get("source", "unknown")
        stats["sources"][source] = stats["sources"].get(source, 0) + 1
        
        input_len = len(item.get("input", ""))
        output_len = len(item.get("output", ""))
        input_lengths.append(input_len)
        output_lengths.append(output_len)
    
    if input_lengths:
        stats["avg_input_length"] = sum(input_lengths) / len(input_lengths)
        stats["avg_output_length"] = sum(output_lengths) / len(output_lengths)
    
    # ä¿å­˜ç»Ÿè®¡
    with open("finetuning_output/dataset_stats.json", 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    return stats

def create_llamafactory_config():
    """åˆ›å»ºLLaMA-Factoryé…ç½®ç¤ºä¾‹"""
    config = {
        "model_name_or_path": "moonshot-v1-8k",
        "finetuning_type": "lora",
        "dataset_dir": "./finetuning_output",
        "dataset": "medical_finetune_train",
        "cutoff_len": 1024,
        "learning_rate": 5e-5,
        "num_train_epochs": 3,
        "per_device_train_batch_size": 2,
        "gradient_accumulation_steps": 4,
        "lora_rank": 8,
        "lora_alpha": 32,
        "lora_dropout": 0.1,
        "output_dir": "./saves/medical-kimi-lora",
        "fp16": True,
        "plot_loss": True
    }
    
    with open("finetuning_output/llamafactory_config.json", 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2)
    
    print("ğŸ“ LLaMA-Factoryé…ç½®æ–‡ä»¶å·²åˆ›å»º")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç®€åŒ–ç‰ˆKimiå¾®è°ƒæ•°æ®ç”Ÿæˆå™¨")
    print("=" * 50)
    
    try:
        all_training_data = []
        
        # 1. åŠ è½½å’Œè½¬æ¢ç°æœ‰æ•°æ®
        chat_data = load_medical_data()
        if chat_data:
            alpaca_data = convert_to_alpaca_format(chat_data)
            all_training_data.extend(alpaca_data)
        
        # 2. ç”Ÿæˆå¢å¼ºQAå¯¹ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæ¡ˆä¾‹ï¼‰
        if chat_data and len(chat_data) > 0:
            sample_case = chat_data[0]["messages"][0]["content"]
            enhanced_qa = generate_enhanced_qa(sample_case)
            all_training_data.extend(enhanced_qa)
        
        # 3. ç”Ÿæˆä¸“ä¸šé¢†åŸŸQAå¯¹
        domain_qa = create_domain_qa()
        all_training_data.extend(domain_qa)
        
        if all_training_data:
            # æ‰“ä¹±æ•°æ®
            random.shuffle(all_training_data)
            
            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
            split_idx = int(len(all_training_data) * 0.8)
            train_data = all_training_data[:split_idx]
            val_data = all_training_data[split_idx:]
            
            # ä¿å­˜æ•°æ®
            train_file = save_jsonl(train_data, "medical_finetune_train.jsonl")
            val_file = save_jsonl(val_data, "medical_finetune_val.jsonl")
            save_jsonl(all_training_data, "medical_finetune_all.jsonl")
            
            # ç”Ÿæˆç»Ÿè®¡å’Œé…ç½®
            stats = generate_stats(all_training_data)
            create_llamafactory_config()
            
            print("\n" + "ğŸ‰" * 30)
            print("å¾®è°ƒæ•°æ®ç”Ÿæˆå®Œæˆï¼")
            print("ğŸ‰" * 30)
            
            print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
            print(f"   è®­ç»ƒé›†: {len(train_data)} æ¡")
            print(f"   éªŒè¯é›†: {len(val_data)} æ¡")
            print(f"   å¹³å‡è¾“å…¥é•¿åº¦: {stats['avg_input_length']:.1f} å­—ç¬¦")
            print(f"   å¹³å‡è¾“å‡ºé•¿åº¦: {stats['avg_output_length']:.1f} å­—ç¬¦")
            
            print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
            print(f"   ğŸ“„ {train_file}")
            print(f"   ğŸ“„ {val_file}")
            print(f"   ğŸ“Š finetuning_output/dataset_stats.json")
            print(f"   âš™ï¸ finetuning_output/llamafactory_config.json")
            
            print(f"\nğŸš€ ä¸‹ä¸€æ­¥:")
            print(f"   1. æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®è´¨é‡")
            print(f"   2. æ ¹æ®éœ€è¦è°ƒæ•´é…ç½®æ–‡ä»¶")
            print(f"   3. ä½¿ç”¨LLaMA-Factoryæˆ–å…¶ä»–æ¡†æ¶è¿›è¡Œå¾®è°ƒ")
            
        else:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ•°æ®")
            
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

